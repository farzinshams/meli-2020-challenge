{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "from datetime import datetime\n",
    "from collections import Counter\n",
    "from joblib import Parallel, delayed, parallel_backend\n",
    "import random\n",
    "import gc\n",
    "from scipy.sparse import csr_matrix, lil_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy.stats import mode\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import ComplementNB, MultinomialNB, CategoricalNB, GaussianNB\n",
    "import re\n",
    "\n",
    "import nltk\n",
    "stopwords = nltk.corpus.stopwords.words('portuguese')\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import figure\n",
    "pd.set_option('display.max_rows', 100)\n",
    "pd.set_option('display.max_columns', 100)\n",
    "pd.set_option('display.float_format', lambda x: '%.2f' % x)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_columns', 300)\n",
    "from textblob import TextBlob\n",
    "from sklearn.model_selection import StratifiedKFold, KFold\n",
    "pd.options.display.float_format = \"{:.4f}\".format\n",
    "from datetime import datetime\n",
    "import dateutil.parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import lightgbm as lgb\n",
    "import bottleneck\n",
    "from sklearn.metrics.pairwise import pairwise_distances, pairwise_distances_argmin\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ml_utils import *\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### READ ORIGINAL FILES AND SAVE THEM AS PICKLE \n",
    "\n",
    "# i = []\n",
    "# with open('item_data.jl', 'rb') as f:\n",
    "#     for item in tqdm(json_lines.reader(f)):\n",
    "#         i.append(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2102277/2102277 [00:02<00:00, 823023.48it/s]\n"
     ]
    }
   ],
   "source": [
    "items = pd.read_pickle('./items')\n",
    "train = pd.read_pickle('./train')\n",
    "indices = train.index.values\n",
    "\n",
    "domain = items[['item_id', 'domain_id']].set_index('item_id').to_dict()['domain_id']\n",
    "domain_inverse = items.groupby('domain_id').agg({'item_id': 'unique'}).to_dict()['item_id']\n",
    "items['item_id'] = items['item_id'].astype('O')\n",
    "items['price'] = items['price'].astype(np.float32)\n",
    "\n",
    "item_info = {}\n",
    "for item_id, price, condition in tqdm(items[['item_id', 'price', 'condition']].values):\n",
    "    item_info[item_id] = (price, 1 if condition == 'new' else 0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test = pd.read_pickle('./test')\n",
    "\n",
    "train, test, indices_train, indices_test = train_test_split(train, indices, test_size=0.20, random_state=15)\n",
    "y_test = test.item_bought.values\n",
    "y_test_domain = np.array([domain[i] for i in y_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = train.item_bought.values\n",
    "y_train_domain = np.array([domain[i] for i in y_train])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "uh_train = list(train.user_history.values)\n",
    "uh_test = list(test.user_history.values)\n",
    "uh = uh_train + uh_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 82633/82633 [00:00<00:00, 93659.86it/s]\n",
      "100%|██████████| 330530/330530 [00:03<00:00, 96742.59it/s] \n"
     ]
    }
   ],
   "source": [
    "test_items = []\n",
    "for uh in tqdm(test.user_history.values):\n",
    "    for i in uh:\n",
    "        if isinstance(i['event_info'], int):\n",
    "            test_items.append(i['event_info'])\n",
    "            \n",
    "# test_items = {domain[i] for i in test_items}\n",
    "test_items = set(test_items)\n",
    "\n",
    "train_items = []\n",
    "for uh in tqdm(train.user_history.values):\n",
    "    for i in uh:\n",
    "        if isinstance(i['event_info'], int):\n",
    "            train_items.append(i['event_info'])\n",
    "            \n",
    "# train_items = {domain[i] for i in train_items}\n",
    "train_items = set(train_items)\n",
    "\n",
    "\n",
    "domain_train_items = {domain[i] for i in train_items}\n",
    "domain_test_items = {domain[i] for i in test_items}\n",
    "\n",
    "target_train_items = set(train.item_bought.values)\n",
    "\n",
    "# target_test_items = set(test.item_bought.values)\n",
    "\n",
    "\n",
    "domain_inverse_sorted = items[items.item_id.isin(train.item_bought.values)].groupby('domain_id').agg({'item_id': 'unique'}).to_dict()['item_id']\n",
    "\n",
    "vc = train.item_bought.value_counts()\n",
    "domain_inverse_sorted = {k: list(vc.loc[v].sort_values(ascending=False).index) for k, v in domain_inverse_sorted.items()}\n",
    "domain_inverse_sorted_count = {k: list(vc.loc[v].sort_values(ascending=False).values) for k, v in domain_inverse_sorted.items()}\n",
    "domain_inverse_sorted_score = {k: [i/sum(v) for i in v] for k, v in domain_inverse_sorted_count.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 330530/330530 [00:05<00:00, 56798.10it/s]\n",
      "100%|██████████| 82633/82633 [00:02<00:00, 40441.04it/s]\n"
     ]
    }
   ],
   "source": [
    "# Getting viewed history\n",
    "\n",
    "uh = train.user_history.values\n",
    "views_train = []\n",
    "for l in tqdm(uh):\n",
    "    current = []\n",
    "    for d in l[::-1]:\n",
    "        event_info = d['event_info']\n",
    "        if isinstance(event_info, int):\n",
    "            current.append(event_info)\n",
    "    views_train.append(current)\n",
    "\n",
    "# Getting viewed history\n",
    "\n",
    "uh = test.user_history.values\n",
    "views_test = []\n",
    "for l in tqdm(uh):\n",
    "    current = []\n",
    "    for d in l[::-1]:\n",
    "        event_info = d['event_info']\n",
    "        if isinstance(event_info, int):\n",
    "            current.append(event_info)\n",
    "    views_test.append(current)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "views = views_train + views_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.preprocessing import LabelEncoder\n",
    "# unique_domains = list(set(np.concatenate((y_train_domain, y_test_domain))))\n",
    "# le = LabelEncoder()\n",
    "# le.fit(unique_domains)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Domains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 330530/330530 [00:09<00:00, 33483.48it/s]\n",
      "100%|██████████| 82633/82633 [00:01<00:00, 48322.30it/s]\n"
     ]
    }
   ],
   "source": [
    "domains = []\n",
    "uh = train.user_history.values\n",
    "for l in tqdm(uh):\n",
    "    current = []\n",
    "    for d in l:\n",
    "        event_info = d['event_info']\n",
    "        if isinstance(event_info, int):\n",
    "            if domain[event_info] in domain_test_items and domain[event_info] != None:\n",
    "                current.append(domain[event_info])\n",
    "    domains.append(current)\n",
    "    \n",
    "\n",
    "domains_test = []\n",
    "uh = test.user_history.values\n",
    "for l in tqdm(uh):\n",
    "    current = []\n",
    "    for d in l:\n",
    "        event_info = d['event_info']\n",
    "        if isinstance(event_info, int):\n",
    "            if domain[event_info] in domain_train_items and domain[event_info] != None:\n",
    "                current.append(domain[event_info])\n",
    "    domains_test.append(current)\n",
    "    \n",
    "domains_total = domains + domains_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = get_csr_matrix(domains_total).astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_dm = ComplementNB(alpha=102)\n",
    "clf_dm.fit(x[:len(train)], y_train_domain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parallelize_predict_proba(clf, x):\n",
    "\n",
    "    def predict(clf, x_loc):\n",
    "        p = clf.predict_proba(x_loc)\n",
    "        proba = np.sort(p, axis=1)[:, :-11:-1]\n",
    "        indices = p.argsort(axis=1)[:, :-11:-1]\n",
    "        preds_10 = []\n",
    "        for i in tqdm(indices):\n",
    "            preds_10.append(list(clf.classes_[i]))\n",
    "        return proba, preds_10\n",
    "    \n",
    "    chunks_locks = np.array_split(np.arange(x.shape[0], dtype=int), 60)\n",
    "    \n",
    "    with parallel_backend('loky', n_jobs=60):\n",
    "        pool_result = Parallel(verbose=5)(delayed(predict)(clf, x[loc]) for loc in chunks_locks)\n",
    "        \n",
    "    proba = []\n",
    "    for i in tqdm(pool_result):\n",
    "        proba.append(i[0])\n",
    "    proba = np.concatenate(tuple(proba), axis=0)\n",
    "    \n",
    "    preds = []\n",
    "    for i in tqdm(pool_result):\n",
    "        preds.extend(i[1])\n",
    "        \n",
    "    return proba, preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=60)]: Using backend LokyBackend with 60 concurrent workers.\n",
      "[Parallel(n_jobs=60)]: Done   6 out of  60 | elapsed:   27.4s remaining:  4.1min\n",
      "[Parallel(n_jobs=60)]: Done  19 out of  60 | elapsed:   29.9s remaining:  1.1min\n",
      "[Parallel(n_jobs=60)]: Done  32 out of  60 | elapsed:   32.0s remaining:   28.0s\n",
      "[Parallel(n_jobs=60)]: Done  45 out of  60 | elapsed:   37.2s remaining:   12.4s\n",
      "[Parallel(n_jobs=60)]: Done  58 out of  60 | elapsed:   39.3s remaining:    1.4s\n",
      "[Parallel(n_jobs=60)]: Done  60 out of  60 | elapsed:   39.6s finished\n",
      "100%|██████████| 60/60 [00:00<00:00, 193285.90it/s]\n",
      "100%|██████████| 60/60 [00:00<00:00, 2429.65it/s]\n"
     ]
    }
   ],
   "source": [
    "proba_domain, preds_domain = parallelize_predict_proba(clf_dm, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('./proba_domain_finaltest.pickle', 'wb') as handle:\n",
    "#     pickle.dump(proba_domain, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "# with open('./preds_domain_finaltest.pickle', 'wb') as handle:\n",
    "#     pickle.dump(preds_domain, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "# with open('./proba_domain_finaltest.pickle', 'rb') as f:\n",
    "#     proba_domain = pickle.load(f)\n",
    "\n",
    "# with open('./preds_domain_finaltest.pickle', 'rb') as f:\n",
    "#     preds_domain = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('./proba_domain.pickle', 'wb') as handle:\n",
    "#     pickle.dump(proba_domain, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "# with open('./preds_domain.pickle', 'wb') as handle:\n",
    "#     pickle.dump(preds_domain, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "# with open('./proba_domain.pickle', 'rb') as f:\n",
    "#     proba_domain = pickle.load(f)\n",
    "\n",
    "# with open('./preds_domain.pickle', 'rb') as f:\n",
    "#     preds_domain = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3794609901613157"
      ]
     },
     "execution_count": 361,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_test_domain, [i[0] for i in preds_domain_test])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text\n",
    "\n",
    " Pega só o primeiro palpite do domínio. Acurácias: 1o palipite: 23%, 2o palipite: 6%, 3o: 3% (no conjunto teste)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 330530/330530 [00:05<00:00, 62571.01it/s] \n",
      "100%|██████████| 82633/82633 [00:00<00:00, 111400.01it/s]\n"
     ]
    }
   ],
   "source": [
    "def get_queries(df):\n",
    "    user_history = df.user_history.values\n",
    "    queries = []\n",
    "    for uh in tqdm(user_history):\n",
    "        s = []\n",
    "        for d in uh:\n",
    "            if d['event_type'] == 'search':\n",
    "                s.append(d['event_info'])\n",
    "        queries.append(s)\n",
    "    queries = [' '.join(list(set(i))).lower() for i in queries]\n",
    "    return queries\n",
    "\n",
    "queries = get_queries(train)\n",
    "\n",
    "queries_test = get_queries(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    return text\n",
    "\n",
    "cv = CountVectorizer(stop_words=stopwords, preprocessor=preprocess_text, max_df=0.2, min_df=0.0001, ngram_range=(1,2))\n",
    "\n",
    "bag_of_words = cv.fit_transform(queries + queries_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ComplementNB(alpha=100)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = ComplementNB(alpha=100)\n",
    "clf.fit(bag_of_words[:len(train)], y_train_domain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parallelize_predict_proba(clf, x):\n",
    "\n",
    "    def predict(clf, x_loc):\n",
    "        p = clf.predict_proba(x_loc)\n",
    "        proba = np.sort(p, axis=1)[:, :-11:-1]\n",
    "        indices = p.argsort(axis=1)[:, :-11:-1]\n",
    "        preds_10 = []\n",
    "        for i in tqdm(indices):\n",
    "            preds_10.append(list(clf.classes_[i]))\n",
    "        return proba, preds_10\n",
    "    \n",
    "    chunks_locks = np.array_split(np.arange(x.shape[0], dtype=int), 60)\n",
    "    \n",
    "    with parallel_backend('loky', n_jobs=60):\n",
    "        pool_result = Parallel(verbose=5)(delayed(predict)(clf, x[loc]) for loc in chunks_locks)\n",
    "        \n",
    "    proba = []\n",
    "    for i in tqdm(pool_result):\n",
    "        proba.append(i[0])\n",
    "    proba = np.concatenate(tuple(proba), axis=0)\n",
    "    \n",
    "    preds = []\n",
    "    for i in tqdm(pool_result):\n",
    "        preds.extend(i[1])\n",
    "        \n",
    "    return proba, preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=60)]: Using backend LokyBackend with 60 concurrent workers.\n",
      "[Parallel(n_jobs=60)]: Done   6 out of  60 | elapsed:   16.9s remaining:  2.5min\n",
      "[Parallel(n_jobs=60)]: Done  19 out of  60 | elapsed:   19.9s remaining:   42.9s\n",
      "[Parallel(n_jobs=60)]: Done  32 out of  60 | elapsed:   22.2s remaining:   19.5s\n",
      "[Parallel(n_jobs=60)]: Done  45 out of  60 | elapsed:   24.6s remaining:    8.2s\n",
      "[Parallel(n_jobs=60)]: Done  58 out of  60 | elapsed:   30.7s remaining:    1.1s\n",
      "[Parallel(n_jobs=60)]: Done  60 out of  60 | elapsed:   31.1s finished\n",
      "100%|██████████| 60/60 [00:00<00:00, 127615.74it/s]\n",
      "100%|██████████| 60/60 [00:00<00:00, 1630.30it/s]\n"
     ]
    }
   ],
   "source": [
    "proba_text, preds_text = parallelize_predict_proba(clf, bag_of_words)\n",
    "# proba_text, preds_text = parallelize_predict_proba(clf, bag_of_words[len(train):])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('./proba_text_finaltest.pickle', 'wb') as handle:\n",
    "#     pickle.dump(proba_text, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "# with open('./preds_text_finaltest.pickle', 'wb') as handle:\n",
    "#     pickle.dump(preds_text, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "# with open('./proba_text_finaltest.pickle', 'rb') as f:\n",
    "#     proba_text = pickle.load(f)\n",
    "\n",
    "# with open('./preds_text_finaltest.pickle', 'rb') as f:\n",
    "#     preds_text = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('./proba_text.pickle', 'wb') as handle:\n",
    "#     pickle.dump(proba_domain, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "# with open('./preds_text.pickle', 'wb') as handle:\n",
    "#     pickle.dump(preds_domain, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "# with open('./proba_text.pickle', 'rb') as f:\n",
    "#     proba_text = pickle.load(f)\n",
    "\n",
    "# with open('./preds_text.pickle', 'rb') as f:\n",
    "#     preds_text = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_item_score(item, domain):\n",
    "    try:\n",
    "        index = domain_inverse_sorted[domain].index(item)\n",
    "        return domain_inverse_sorted_score[domain][index]\n",
    "    except:\n",
    "        return 0\n",
    "\n",
    "def generate_datapoint(v, y, domains_10, text_10, proba_domains_10, proba_text_10):\n",
    "    candidates = np.unique(v + list(domain_inverse_sorted[domains_10[0]][:10]) + list(domain_inverse_sorted[domains_10[1]][:2]))\n",
    "\n",
    "\n",
    "    nunique = len(np.unique(v))\n",
    "    lenn = len(v)\n",
    "    counts = counter(v)\n",
    "    rec_score = get_recurrent_datapoint(v)\n",
    "    prices = [0] if len(v) == 0 else [item_info[i][0] for i in v]\n",
    "    maxx = np.max(prices)\n",
    "    minn = np.min(prices)\n",
    "    amp = maxx - minn\n",
    "    \n",
    "    try:\n",
    "        unique_domains = get_unique_domains(v)\n",
    "    except:\n",
    "        unique_domains = 1\n",
    "\n",
    "    datapoints = []\n",
    "    for item in candidates:\n",
    "        datapoints.append({\n",
    "            'is_first': 1 if len(v) > 0 and item == v[0] else 0,\n",
    "            'is_last': 1 if len(v) > 0 and item == v[-1] else 0,\n",
    "            'count': counts.get(item, 0),\n",
    "            'nunique': nunique,\n",
    "            'max_price_viewed': maxx,\n",
    "            'min_price_viewed': minn,\n",
    "            'amp_price': amp,\n",
    "            'lenn': lenn,\n",
    "            'rec_score': rec_score.get(item, 0),\n",
    "            'prob_domain': get_domain_score(domain[item], domains_10, proba_domains_10),\n",
    "            'prob_text': get_domain_score(domain[item], text_10, proba_text_10),\n",
    "            'prob_item': get_item_score(item, domain[item]),\n",
    "            'unique_domains': unique_domains,\n",
    "            'price': item_info[item][0],\n",
    "            'condition': item_info[item][1],\n",
    "            'target': 1 if y == item else 0\n",
    "        })\n",
    "\n",
    "    datapoints = pd.DataFrame().from_records(datapoints)\n",
    "    return datapoints\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100000/100000 [03:10<00:00, 524.35it/s]\n"
     ]
    }
   ],
   "source": [
    "datapoints = []\n",
    "for i in tqdm(range(len(train[:100000]))):\n",
    "    datapoints.append(generate_datapoint(views[i], y_train[i], preds_domain[i], preds_text[i], proba_domain[i], proba_text[i]))\n",
    "\n",
    "datapoints = pd.concat(datapoints).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dp_fe(df):\n",
    "    df['perc'] = df['count']/df['lenn']\n",
    "    df['prob_item_domain'] = df['prob_domain']*df['prob_item']\n",
    "    df['prob_item_text'] = df['prob_text']*df['prob_item']\n",
    "    return df\n",
    "\n",
    "datapoints = dp_fe(datapoints)\n",
    "\n",
    "datapoints = undersample(datapoints)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# datapoints.to_pickle('./datapoints.pickle')\n",
    "\n",
    "# with open('./datapoints.pickle', 'rb') as f:\n",
    "#     datapoints = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "dp_x = datapoints.drop('target', axis=1)\n",
    "dp_y = datapoints['target']\n",
    "\n",
    "dp_x_train, dp_x_test, dp_y_train, dp_y_test = train_test_split(dp_x, dp_y, test_size=0.25, random_state=42, stratify=dp_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>is_first</th>\n",
       "      <th>is_last</th>\n",
       "      <th>count</th>\n",
       "      <th>nunique</th>\n",
       "      <th>max_price_viewed</th>\n",
       "      <th>min_price_viewed</th>\n",
       "      <th>amp_price</th>\n",
       "      <th>lenn</th>\n",
       "      <th>rec_score</th>\n",
       "      <th>prob_domain</th>\n",
       "      <th>prob_text</th>\n",
       "      <th>prob_item</th>\n",
       "      <th>unique_domains</th>\n",
       "      <th>price</th>\n",
       "      <th>condition</th>\n",
       "      <th>target</th>\n",
       "      <th>perc</th>\n",
       "      <th>prob_item_domain</th>\n",
       "      <th>prob_item_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1887446</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>37</td>\n",
       "      <td>3999.8999</td>\n",
       "      <td>18.0000</td>\n",
       "      <td>3981.8999</td>\n",
       "      <td>51</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.0018</td>\n",
       "      <td>0.0211</td>\n",
       "      <td>1</td>\n",
       "      <td>179.0000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0211</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1668470</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>14</td>\n",
       "      <td>461.7900</td>\n",
       "      <td>62.6900</td>\n",
       "      <td>399.1000</td>\n",
       "      <td>25</td>\n",
       "      <td>0</td>\n",
       "      <td>0.9994</td>\n",
       "      <td>0.0007</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1</td>\n",
       "      <td>139.9900</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0400</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1540843</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2099.0000</td>\n",
       "      <td>1499.0000</td>\n",
       "      <td>600.0000</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0157</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0016</td>\n",
       "      <td>1</td>\n",
       "      <td>2099.0000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.3333</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1082758</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>87.3000</td>\n",
       "      <td>87.3000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0004</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.1220</td>\n",
       "      <td>1</td>\n",
       "      <td>774.0000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1475166</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>22</td>\n",
       "      <td>220000.0000</td>\n",
       "      <td>15.0000</td>\n",
       "      <td>219985.0000</td>\n",
       "      <td>51</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1</td>\n",
       "      <td>17.9000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0196</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1173379</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>27</td>\n",
       "      <td>9199.0000</td>\n",
       "      <td>40.0000</td>\n",
       "      <td>9159.0000</td>\n",
       "      <td>34</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0053</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0825</td>\n",
       "      <td>1</td>\n",
       "      <td>279.9900</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0004</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1517143</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>7889.8799</td>\n",
       "      <td>549.0000</td>\n",
       "      <td>7340.8799</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0.8105</td>\n",
       "      <td>0.0014</td>\n",
       "      <td>0.0079</td>\n",
       "      <td>1</td>\n",
       "      <td>899.0000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0064</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1665134</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>159.4000</td>\n",
       "      <td>19.7500</td>\n",
       "      <td>139.6500</td>\n",
       "      <td>23</td>\n",
       "      <td>0</td>\n",
       "      <td>0.8222</td>\n",
       "      <td>0.0028</td>\n",
       "      <td>0.0438</td>\n",
       "      <td>1</td>\n",
       "      <td>119.9000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0360</td>\n",
       "      <td>0.0001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1087598</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>57</td>\n",
       "      <td>87000.0000</td>\n",
       "      <td>56.7000</td>\n",
       "      <td>86943.3000</td>\n",
       "      <td>86</td>\n",
       "      <td>0</td>\n",
       "      <td>0.2088</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>0.0568</td>\n",
       "      <td>1</td>\n",
       "      <td>43.9900</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0119</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1565127</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>9999.0000</td>\n",
       "      <td>49.9900</td>\n",
       "      <td>9949.0100</td>\n",
       "      <td>36</td>\n",
       "      <td>0</td>\n",
       "      <td>0.1224</td>\n",
       "      <td>0.0133</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1</td>\n",
       "      <td>1839.0000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0278</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>141764 rows × 19 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         is_first  is_last  count  nunique  max_price_viewed  \\\n",
       "1887446         0        0      0       37         3999.8999   \n",
       "1668470         0        0      1       14          461.7900   \n",
       "1540843         0        0      1        3         2099.0000   \n",
       "1082758         0        0      0        1           87.3000   \n",
       "1475166         0        0      1       22       220000.0000   \n",
       "...           ...      ...    ...      ...               ...   \n",
       "1173379         0        0      0       27         9199.0000   \n",
       "1517143         0        0      0        6         7889.8799   \n",
       "1665134         0        0      0       11          159.4000   \n",
       "1087598         0        0      0       57        87000.0000   \n",
       "1565127         0        0      1       12         9999.0000   \n",
       "\n",
       "         min_price_viewed   amp_price  lenn  rec_score  prob_domain  \\\n",
       "1887446           18.0000   3981.8999    51          0       1.0000   \n",
       "1668470           62.6900    399.1000    25          0       0.9994   \n",
       "1540843         1499.0000    600.0000     3          0       0.0157   \n",
       "1082758           87.3000      0.0000     1          0       0.0004   \n",
       "1475166           15.0000 219985.0000    51          0       0.0000   \n",
       "...                   ...         ...   ...        ...          ...   \n",
       "1173379           40.0000   9159.0000    34          0       0.0053   \n",
       "1517143          549.0000   7340.8799     9          0       0.8105   \n",
       "1665134           19.7500    139.6500    23          0       0.8222   \n",
       "1087598           56.7000  86943.3000    86          0       0.2088   \n",
       "1565127           49.9900   9949.0100    36          0       0.1224   \n",
       "\n",
       "         prob_text  prob_item  unique_domains     price  condition  target  \\\n",
       "1887446     0.0018     0.0211               1  179.0000          1       0   \n",
       "1668470     0.0007     0.0000               1  139.9900          1       0   \n",
       "1540843     0.0000     0.0016               1 2099.0000          1       1   \n",
       "1082758     0.0000     0.1220               1  774.0000          1       0   \n",
       "1475166     0.0000     0.0000               1   17.9000          1       0   \n",
       "...            ...        ...             ...       ...        ...     ...   \n",
       "1173379     0.0000     0.0825               1  279.9900          1       0   \n",
       "1517143     0.0014     0.0079               1  899.0000          1       0   \n",
       "1665134     0.0028     0.0438               1  119.9000          1       0   \n",
       "1087598     0.0005     0.0568               1   43.9900          1       0   \n",
       "1565127     0.0133     0.0000               1 1839.0000          1       0   \n",
       "\n",
       "          perc  prob_item_domain  prob_item_text  \n",
       "1887446 0.0000            0.0211          0.0000  \n",
       "1668470 0.0400            0.0000          0.0000  \n",
       "1540843 0.3333            0.0000          0.0000  \n",
       "1082758 0.0000            0.0001          0.0000  \n",
       "1475166 0.0196            0.0000          0.0000  \n",
       "...        ...               ...             ...  \n",
       "1173379 0.0000            0.0004          0.0000  \n",
       "1517143 0.0000            0.0064          0.0000  \n",
       "1665134 0.0000            0.0360          0.0001  \n",
       "1087598 0.0000            0.0119          0.0000  \n",
       "1565127 0.0278            0.0000          0.0000  \n",
       "\n",
       "[141764 rows x 19 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datapoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/coder/.local/lib/python3.7/site-packages/lightgbm/engine.py:151: UserWarning: Found `num_boost_round` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 26581, number of negative: 79742\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003598 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3022\n",
      "[LightGBM] [Info] Number of data points in the train set: 106323, number of used features: 17\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.250002 -> initscore=-1.098600\n",
      "[LightGBM] [Info] Start training from score -1.098600\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[10]\tvalid_0's binary_logloss: 0.300173\n",
      "[20]\tvalid_0's binary_logloss: 0.258582\n",
      "[30]\tvalid_0's binary_logloss: 0.247211\n",
      "[40]\tvalid_0's binary_logloss: 0.243633\n",
      "[50]\tvalid_0's binary_logloss: 0.242283\n",
      "[60]\tvalid_0's binary_logloss: 0.241764\n",
      "[70]\tvalid_0's binary_logloss: 0.241587\n",
      "[80]\tvalid_0's binary_logloss: 0.241564\n",
      "[90]\tvalid_0's binary_logloss: 0.241495\n",
      "[100]\tvalid_0's binary_logloss: 0.241563\n"
     ]
    }
   ],
   "source": [
    "lr = 0.12\n",
    "lr_decay = 0.98\n",
    "\n",
    "\n",
    "params = {\n",
    "    'boosting_type': 'gbdt',\n",
    "    'objective': 'binary',\n",
    "    'metric': 'binary_logloss',\n",
    "    'num_leaves': 512,\n",
    "    'feature_fraction': 0.9,\n",
    "    'bagging_fraction': 0.8,\n",
    "    'lambda_l2': 1.2, #0.65,\n",
    "    'lambda_l1': 1.2, #0.65,\n",
    "    'bagging_freq': 5,\n",
    "    'num_boost_round': 100,\n",
    "    'verbose': 1\n",
    "}\n",
    "\n",
    "\n",
    "dtrain = lgb.Dataset(dp_x_train, dp_y_train)\n",
    "dvalid = lgb.Dataset(dp_x_test, dp_y_test, reference=dtrain)\n",
    "\n",
    "bst = lgb.train(\n",
    "    params, dtrain, valid_sets=dvalid, verbose_eval=10,\n",
    "    callbacks=[lgb.reset_parameter(learning_rate=lambda current_round: lr*(lr_decay**current_round))],\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dp_fe(df):\n",
    "    df['perc'] = df['count']/df['lenn']\n",
    "    df['prob_item_domain'] = df['prob_domain']*df['prob_item']\n",
    "    df['prob_item_text'] = df['prob_text']*df['prob_item']\n",
    "    \n",
    "    return df\n",
    "\n",
    "def generate_datapoint_without_y(candidates, v, domains_10, text_10, proba_domains_10, proba_text_10):\n",
    "\n",
    "    nunique = len(np.unique(v))\n",
    "    lenn = len(v)\n",
    "    counts = counter(v)\n",
    "    rec_score = get_recurrent_datapoint(v)\n",
    "    prices = [0] if len(v) == 0 else [item_info[i][0] for i in v]\n",
    "    maxx = np.max(prices)\n",
    "    minn = np.min(prices)\n",
    "    amp = maxx - minn\n",
    "    \n",
    "    try:\n",
    "        unique_domains = get_unique_domains(v)\n",
    "    except:\n",
    "        unique_domains = 1\n",
    "\n",
    "    datapoints = []\n",
    "    for item in candidates:\n",
    "        datapoints.append({\n",
    "            'is_first': 1 if len(v) > 0 and item == v[0] else 0,\n",
    "            'is_last': 1 if len(v) > 0 and item == v[-1] else 0,\n",
    "            'count': counts.get(item, 0),\n",
    "            'nunique': nunique,\n",
    "            'max_price_viewed': maxx,\n",
    "            'min_price_viewed': minn,\n",
    "            'amp_price': amp,\n",
    "            'lenn': lenn,\n",
    "            'rec_score': rec_score.get(item, 0),\n",
    "            'prob_domain': get_domain_score(domain[item], domains_10, proba_domains_10),\n",
    "            'prob_text': get_domain_score(domain[item], text_10, proba_text_10),\n",
    "            'prob_item': get_item_score(item, domain[item]),\n",
    "            'unique_domains': unique_domains,\n",
    "            'price': item_info[item][0],\n",
    "            'condition': item_info[item][1]\n",
    "        })\n",
    "\n",
    "    datapoints = pd.DataFrame().from_records(datapoints)\n",
    "    datapoints = dp_fe(datapoints)\n",
    "    return datapoints\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def p(lenn_test, preds_domain, preds_text, proba_domain, proba_text, views):\n",
    "\n",
    "    outputs = []\n",
    "    for idx in tqdm(range(lenn_test)):\n",
    "        v = views[idx]\n",
    "        domains_10 = preds_domain[idx]\n",
    "        texts_10 = preds_text[idx]\n",
    "        proba_domains_10 = proba_domain[idx]\n",
    "        proba_text_10 = proba_text[idx]\n",
    "\n",
    "        output = []\n",
    "        j = 0\n",
    "        while len(output) < 10:\n",
    "            if len(v) > 0:\n",
    "#             if proba_domains_10[j] >= 0.0003808080808080808:\n",
    "                current_domain = domains_10[j]\n",
    "            else:\n",
    "                current_domain = texts_10[j]\n",
    "            current_domain_items = domain_inverse_sorted[current_domain]\n",
    "\n",
    "            views_of_domain = unique([i for i in v if domain[i] == current_domain])\n",
    "\n",
    "            current_domain_items = [i for i in current_domain_items if i in views_of_domain]\n",
    "\n",
    "            output.extend(current_domain_items)\n",
    "\n",
    "            j += 1\n",
    "            if j == 10:\n",
    "                k = 0\n",
    "                while len(output) < 10:\n",
    "                    if len(v) > 0:\n",
    "                        current_domain = domains_10[k]\n",
    "                    else:\n",
    "                        current_domain = texts_10[k]\n",
    "                    current_domain_items = domain_inverse_sorted[current_domain]\n",
    "\n",
    "                    for i in current_domain_items:\n",
    "                        if i not in output:\n",
    "                            output.append(i)\n",
    "                    k += 1\n",
    "\n",
    "        output = output[:10]\n",
    "\n",
    "        datapoints = generate_datapoint_without_y(output, v, domains_10, texts_10, proba_domains_10, proba_text_10)\n",
    "        preds = bst.predict(datapoints, num_iteration=bst.best_iteration)\n",
    "        args = preds.argsort()\n",
    "        output = list(np.array(output)[args[::-1]][:10])\n",
    "\n",
    "        outputs.append(output)\n",
    "        \n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "413163 413163 413163 413163 413163 82633\n"
     ]
    }
   ],
   "source": [
    "print(len(preds_domain), len(preds_text), len(proba_domain), len(proba_text), len(views), len(uh))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:27<00:00, 36.00it/s]\n"
     ]
    }
   ],
   "source": [
    "outputs = p(len(test[:1000]), preds_domain[len(train):], preds_text[len(train):], proba_domain[len(train):], proba_text[len(train):], views[len(train):])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6000/6000 [00:00<00:00, 33312.54it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.2933451058874392"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "average_NDCG(outputs, y_test, domain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6000/6000 [00:00<00:00, 33186.72it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.2933451058874392"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "average_NDCG(outputs, y_test, domain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [00:00<00:00, 28499.24it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.2964340121237794"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "average_NDCG(outputs, y_test, domain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:00<00:00, 28476.12it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.283689159463642"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "average_NDCG(outputs, y_test, domain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:00<00:00, 24474.56it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.2920434383738991"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "average_NDCG(outputs, y_test, domain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:00<00:00, 25805.54it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.2920434383738991"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "average_NDCG(outputs, y_test, domain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:00<00:00, 27623.55it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.2910180853923794"
      ]
     },
     "execution_count": 243,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "average_NDCG(outputs, y_test, domain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:00<00:00, 27173.63it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.28888895067853193"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "average_NDCG(outputs, y_test, domain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:00<00:00, 26570.11it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.2850787020990637"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "average_NDCG(outputs, y_test, domain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [00:00<00:00, 27979.15it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.2861882115066076"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "average_NDCG(outputs, y_test, domain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:00<00:00, 31877.91it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.28614013452515735"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "average_NDCG(outputs, y_test, domain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:00<00:00, 27811.84it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.28601171348072796"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "average_NDCG(outputs, y_test, domain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Format output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in outputs:\n",
    "    assert len(i) == 10\n",
    "    assert len(set(i)) == 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"output_.csv\", \"w\") as f:\n",
    "    for row in outputs:\n",
    "        f.write(\"%s\\n\" % ','.join(str(col) for col in row))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
